name: "Base PRP Template v3 - Implementation-Focused with Precision Standards"ndescription: |nn---nn## Goalnn**Feature Goal**: [Specific, measurable end state of what needs to be built]nn**Deliverable**: [Concrete artifact - API endpoint, service class, integration, etc.]nn**Success Definition**: [How you'll know this is complete and working]nn## User Persona (if applicable)nn**Target User**: [Specific user type - developer, end user, admin, etc.]nn**Use Case**: [Primary scenario when this feature will be used]nn**User Journey**: [Step-by-step flow of how user interacts with this feature]nn**Pain Points Addressed**: [Specific user frustrations this feature solves]nn## Whynn- [Business value and user impact]n- [Integration with existing features]n- [Problems this solves and for whom]nn## Whatnn[User-visible behavior and technical requirements]nn### Success Criteriann- [ ] [Specific measurable outcomes]nn## All Needed Contextnn### Context Completeness Checknn_Before writing this PRP, validate: "If someone knew nothing about this codebase, would they have everything needed to implement this successfully?"_nn### Documentation & Referencesnn```yamln# MUST READ - Include these in your context windown- url: [Complete URL with section anchor]n  why: [Specific methods/concepts needed for implementation]n  critical: [Key insights that prevent common implementation errors]nn- file: [exact/path/to/pattern/file.py]n  why: [Specific pattern to follow - class structure, error handling, etc.]n  pattern: [Brief description of what pattern to extract]n  gotcha: [Known constraints or limitations to avoid]nn- docfile: [PRPs/ai_docs/domain_specific.md]n  why: [Custom documentation for complex library/integration patterns]n  section: [Specific section if document is large]n```nn### Current Codebase tree (run `tree` in the root of the project) to get an overview of the codebasenn```bashnn```nn### Desired Codebase tree with files to be added and responsibility of filenn```bashnn```nn### Known Gotchas of our codebase & Library Quirksnn```pythonn# CRITICAL: [Library name] requires [specific setup]n# Example: FastAPI requires async functions for endpointsn# Example: This ORM doesn't support batch inserts over 1000 recordsn```nn## Implementation Blueprintnn### Data models and structurennCreate the core data models, we ensure type safety and consistency.nn```pythonnExamples:n - orm modelsn - pydantic modelsn - pydantic schemasn - pydantic validatorsnn```nn### Implementation Tasks (ordered by dependencies)nn```yamlnTask 1: CREATE src/models/{domain}_models.pyn  - IMPLEMENT: {SpecificModel}Request, {SpecificModel}Response Pydantic modelsn  - FOLLOW pattern: src/models/existing_model.py (field validation approach)n  - NAMING: CamelCase for classes, snake_case for fieldsn  - PLACEMENT: Domain-specific model file in src/models/nnTask 2: CREATE src/services/{domain}_service.pyn  - IMPLEMENT: {Domain}Service class with async methodsn  - FOLLOW pattern: src/services/database_service.py (service structure, error handling)n  - NAMING: {Domain}Service class, async def create_*, get_*, update_*, delete_* methodsn  - DEPENDENCIES: Import models from Task 1n  - PLACEMENT: Service layer in src/services/nnTask 3: CREATE src/tools/{action}_{resource}.pyn  - IMPLEMENT: MCP tool wrapper calling service methodsn  - FOLLOW pattern: src/tools/existing_tool.py (FastMCP tool structure)n  - NAMING: snake_case file name, descriptive tool function namen  - DEPENDENCIES: Import service from Task 2n  - PLACEMENT: Tool layer in src/tools/nnTask 4: MODIFY src/main.py or src/server.pyn  - INTEGRATE: Register new tool with MCP servern  - FIND pattern: existing tool registrationsn  - ADD: Import and register new tool following existing patternn  - PRESERVE: Existing tool registrations and server configurationnnTask 5: CREATE src/services/tests/test_{domain}_service.pyn  - IMPLEMENT: Unit tests for all service methods (happy path, edge cases, error handling)n  - FOLLOW pattern: src/services/tests/test_existing_service.py (fixture usage, assertion patterns)n  - NAMING: test_{method}_{scenario} function namingn  - COVERAGE: All public methods with positive and negative test casesn  - PLACEMENT: Tests alongside the code they testnnTask 6: CREATE src/tools/tests/test_{action}_{resource}.pyn  - IMPLEMENT: Unit tests for MCP tool functionalityn  - FOLLOW pattern: src/tools/tests/test_existing_tool.py (MCP tool testing approach)n  - MOCK: External service dependenciesn  - COVERAGE: Tool input validation, success responses, error handlingn  - PLACEMENT: Tool tests in src/tools/tests/n```nn### Implementation Patterns & Key Detailsnn```pythonn# Show critical patterns and gotchas - keep concise, focus on non-obvious detailsnn# Example: Service method patternnasync def {domain}_operation(self, request: {Domain}Request) -> {Domain}Response:n    # PATTERN: Input validation first (follow src/services/existing_service.py)n    validated = self.validate_request(request)nn    # GOTCHA: [Library-specific constraint or requirement]n    # PATTERN: Error handling approach (reference existing service pattern)n    # CRITICAL: [Non-obvious requirement or configuration detail]nn    return {Domain}Response(status="success", data=result)nn# Example: MCP tool patternn@app.tool()nasync def {tool_name}({parameters}) -> str:n    # PATTERN: Tool validation and service delegation (see src/tools/existing_tool.py)n    # RETURN: JSON string with standardized response formatn```nn### Integration Pointsnn```yamlnDATABASE:n  - migration: "Add column 'feature_enabled' to users table"n  - index: "CREATE INDEX idx_feature_lookup ON users(feature_id)"nnCONFIG:n  - add to: config/settings.pyn  - pattern: "FEATURE_TIMEOUT = int(os.getenv('FEATURE_TIMEOUT', '30'))"nnROUTES:n  - add to: src/api/routes.pyn  - pattern: "router.include_router(feature_router, prefix='/feature')"n```nn## Validation Loopnn### Level 1: Syntax & Style (Immediate Feedback)nn```bashn# Run after each file creation - fix before proceedingnruff check src/{new_files} --fix     # Auto-format and fix linting issuesnmypy src/{new_files}                 # Type checking with specific filesnruff format src/{new_files}          # Ensure consistent formattingnn# Project-wide validationnruff check src/ --fixnmypy src/nruff format src/nn# Expected: Zero errors. If errors exist, READ output and fix before proceeding.n```nn### Level 2: Unit Tests (Component Validation)nn```bashn# Test each component as it's creatednuv run pytest src/services/tests/test_{domain}_service.py -vnuv run pytest src/tools/tests/test_{action}_{resource}.py -vnn# Full test suite for affected areasnuv run pytest src/services/tests/ -vnuv run pytest src/tools/tests/ -vnn# Coverage validation (if coverage tools available)nuv run pytest src/ --cov=src --cov-report=term-missingnn# Expected: All tests pass. If failing, debug root cause and fix implementation.n```nn### Level 3: Integration Testing (System Validation)nn```bashn# Service startup validationnuv run python main.py &nsleep 3  # Allow startup timenn# Health check validationncurl -f http://localhost:8000/health || echo "Service health check failed"nn# Feature-specific endpoint testingncurl -X POST http://localhost:8000/{your_endpoint} \n  -H "Content-Type: application/json" \n  -d '{"test": "data"}' \n  | jq .  # Pretty print JSON responsenn# MCP server validation (if MCP-based)n# Test MCP tool functionalitynecho '{"method": "tools/call", "params": {"name": "{tool_name}", "arguments": {}}}' | \n  uv run python -m src.mainnn# Database validation (if database integration)n# Verify database schema, connections, migrationsnpsql $DATABASE_URL -c "SELECT 1;" || echo "Database connection failed"nn# Expected: All integrations working, proper responses, no connection errorsn```nn### Level 4: Creative & Domain-Specific Validationnn```bashn# MCP Server Validation Examples:nn# Playwright MCP (for web interfaces)nplaywright-mcp --url http://localhost:8000 --test-user-journeynn# Docker MCP (for containerized services)ndocker-mcp --build --test --cleanupnn# Database MCP (for data operations)ndatabase-mcp --validate-schema --test-queries --check-performancenn# Custom Business Logic Validationn# [Add domain-specific validation commands here]nn# Performance Testing (if performance requirements)nab -n 100 -c 10 http://localhost:8000/{endpoint}nn# Security Scanning (if security requirements)nbandit -r src/nn# Load Testing (if scalability requirements)n# wrk -t12 -c400 -d30s http://localhost:8000/{endpoint}nn# API Documentation Validation (if API endpoints)n# swagger-codegen validate -i openapi.jsonnn# Expected: All creative validations pass, performance meets requirementsn```nn## Final Validation Checklistnn### Technical Validationnn- [ ] All 4 validation levels completed successfullyn- [ ] All tests pass: `uv run pytest src/ -v`n- [ ] No linting errors: `uv run ruff check src/`n- [ ] No type errors: `uv run mypy src/`n- [ ] No formatting issues: `uv run ruff format src/ --check`nn### Feature Validationnn- [ ] All success criteria from "What" section metn- [ ] Manual testing successful: [specific commands from Level 3]n- [ ] Error cases handled gracefully with proper error messagesn- [ ] Integration points work as specifiedn- [ ] User persona requirements satisfied (if applicable)nn### Code Quality Validationnn- [ ] Follows existing codebase patterns and naming conventionsn- [ ] File placement matches desired codebase tree structuren- [ ] Anti-patterns avoided (check against Anti-Patterns section)n- [ ] Dependencies properly managed and importedn- [ ] Configuration changes properly integratednn### Documentation & Deploymentnn- [ ] Code is self-documenting with clear variable/function namesn- [ ] Logs are informative but not verbosen- [ ] Environment variables documented if new ones addednn---nn## Anti-Patterns to Avoidnn- ❌ Don't create new patterns when existing ones workn- ❌ Don't skip validation because "it should work"n- ❌ Don't ignore failing tests - fix themn- ❌ Don't use sync functions in async contextn- ❌ Don't hardcode values that should be confign- ❌ Don't catch all exceptions - be specific